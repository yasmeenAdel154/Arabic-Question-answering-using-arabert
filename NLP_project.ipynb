{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**this code from : https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626**"
      ],
      "metadata": {
        "id": "oMAyddqSQCER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq2S3YD4D-9s",
        "outputId": "83f5911b-5722-4001-bc4b-1b3dfc2cc226"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading from Local CSV File"
      ],
      "metadata": {
        "id": "GKXHd8D3MZ-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lda-Vu4kDcT1",
        "outputId": "0c917b87-8a95-459d-9d07-ee458c98fcf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "1  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "2  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "3  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "4  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "\n",
            "                                          question  \\\n",
            "0                   - من هو جمال أحمد حمزة خاشقجي؟   \n",
            "1        - متى ولد جمال أحمد حمزة خاشقجي وتوفي؟ ال   \n",
            "2      - في أي مدينة ولد جمال أحمد حمزة خاشقجي؟ ال   \n",
            "3   - في أي صحيفة قام بكتابة عمود منذ عام 2017؟ ال   \n",
            "4  - كيف وصفها في الصحف ووسائل الإعلام الدولية؟ ال   \n",
            "\n",
            "                                             answers  \n",
            "0  {'text': array(['صحفي وإعلامي'], dtype=object)...  \n",
            "1  {'text': array(['حمزة خاشقجي (13 أكتوبر 1958، ...  \n",
            "2  {'text': array(['المدينة المنورة'], dtype=obje...  \n",
            "3  {'text': array(['واشنطن بوست'], dtype=object),...  \n",
            "4  {'text': array(['وُصف في الصحف وأجهزة الاعلام ...  \n",
            "0      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "1      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "2      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "3      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "4      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "                             ...                        \n",
            "688    البظر هو المنطقة المثيرة للشهوة الجنسية الأكثر...\n",
            "689    البظر هو المنطقة المثيرة للشهوة الجنسية الأكثر...\n",
            "690    يقول قاموس إكسفورد للغة الإنجليزيّة أن لكلمة \"...\n",
            "691    يقول قاموس إكسفورد للغة الإنجليزيّة أن لكلمة \"...\n",
            "692    يقول قاموس إكسفورد للغة الإنجليزيّة أن لكلمة \"...\n",
            "Name: text, Length: 693, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#put train.csv file in this notebook\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "#pre processing\n",
        "del data[\"id\"]\n",
        "del data[\"title\"]\n",
        "\n",
        "#remeber to convert context in train.csv to text \n",
        "\n",
        "print ( data.head() )\n",
        "print ( data[\"text\"]  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of question and answers: \", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T2ZHKDzF8oB",
        "outputId": "ab809bb1-9b40-4f47-b3ca-d4fff5686fc8"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of question and answers:  693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Chatbot\n",
        "The best part about using these pre-trained models is that you can load the model and its tokenizer in just two simple lines of code. 😲 Isn’t it simply wow? For tasks like text classification, we need to fine-tune BERT on our dataset. But for question answering tasks, we can even use the already trained model and get decent results even when our text is from a completely different domain. To get decent results, we are using a BERT model which is fine-tuned on the SQuAD benchmark.\n",
        "\n",
        "For our task, we will use the BertForQuestionAnswering class from the transformers library."
      ],
      "metadata": {
        "id": "oJhzltZNMtjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('asafaya/bert-base-arabic')\n",
        "tokenizer = BertTokenizer.from_pretrained('asafaya/bert-base-arabic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRf_6bETGyJt",
        "outputId": "055cc168-d442-4440-fa10-cfd95362b748"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asking a Question\n",
        "Let’s randomly select a question number."
      ],
      "metadata": {
        "id": "PbX1sBBjMwbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_num = np.random.randint(0,len(data))\n",
        "question = data[\"question\"][random_num]\n",
        "text = data[\"text\"][random_num]\n",
        "\n",
        "\n",
        "\n",
        "#question = data[\"question\"][1]\n",
        "#text = data[\"text\"][1]\n",
        "\n",
        "print(\"question : \" +question )\n",
        "print(\"text : \" +text )\n",
        "\n",
        "#question = \" ما اسم كلية ياسمين ؟ \"\n",
        "#text = \"ياسمين لديها اخ واحد اسمه محمد عادل و ياسمين في كلية الحاسبات و الذكاء الاصطناعي لديها العديد من الاصدقاء \""
      ],
      "metadata": {
        "id": "xjMfo6AGMz-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5f9e30-2ef4-4d27-9be1-492011a91fe3"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question : ماهي المهنة التي كان يمارسها الفينيقيون؟\n",
            "text : في القدم، سكن الفينيقيون أرض لبنان الحالية مع جزء من أرض سوريا وفلسطين، وهؤلاء قوم ساميون اتخذوا من الملاحة والتجارة مهنة لهم، وازدهرت حضارتهم طيلة 2500 سنة تقريبًا (من حوالي سنة 3000 حتى سنة 539 ق.م). وقد مرّت على لبنان عدّة حضارات وشعوب استقرت فيه منذ عهد الفينيقين، مثل المصريين القدماء، الآشوريين، الفرس، الإغريق، الرومان، الروم البيزنطيين، العرب، الصليبيين، الأتراك العثمانيين، فالفرنسيين.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s tokenize the question and text as a pair."
      ],
      "metadata": {
        "id": "9l7iisiNM-jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))\n",
        "print ( input_ids )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbivNuCGM_1t",
        "outputId": "44a084f6-c268-4ba0-ee10-1954e75ffa64"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 116 tokens.\n",
            "[2, 17490, 18372, 1833, 1841, 12339, 1731, 23547, 14222, 1733, 221, 3, 1725, 3955, 219, 7088, 23547, 14222, 1733, 3648, 2851, 7252, 1776, 4412, 1726, 3648, 3468, 24067, 219, 18637, 6954, 9801, 1733, 8795, 1751, 1726, 14002, 12923, 16611, 2819, 219, 9254, 2986, 1920, 5893, 21923, 14338, 8361, 1034, 2634, 6527, 12, 1726, 5543, 2634, 22423, 2123, 2634, 7847, 1039, 245, 18, 248, 13, 18, 2269, 3484, 1747, 2851, 3630, 5893, 1912, 12536, 1870, 22122, 1013, 2229, 2597, 5609, 23547, 14222, 1017, 219, 2358, 8916, 25946, 219, 2324, 24130, 219, 17551, 219, 3488, 2028, 219, 21225, 219, 7903, 2062, 3573, 28683, 219, 3153, 219, 17084, 2696, 219, 20536, 14016, 1724, 219, 22314, 1918, 16084, 18, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see how many tokens this question and text pair have."
      ],
      "metadata": {
        "id": "1aPyZ1d4NOMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at what our tokenizer is doing, let’s just print out the tokens and their IDs."
      ],
      "metadata": {
        "id": "kqzJHATRNkM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk0Z2hfYNsaz",
        "outputId": "5f974644-a857-47c4-d2ce-f64c1475fd6b"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]          2\n",
            "ماهي      17,490\n",
            "المهنة    18,372\n",
            "التي       1,833\n",
            "كان        1,841\n",
            "يمارس     12,339\n",
            "##ها       1,731\n",
            "الفين     23,547\n",
            "##يقي     14,222\n",
            "##ون       1,733\n",
            "؟            221\n",
            "[SEP]          3\n",
            "في         1,725\n",
            "القدم      3,955\n",
            "،            219\n",
            "سكن        7,088\n",
            "الفين     23,547\n",
            "##يقي     14,222\n",
            "##ون       1,733\n",
            "ارض        3,648\n",
            "لبنان      2,851\n",
            "الحالية    7,252\n",
            "مع         1,776\n",
            "جزء        4,412\n",
            "من         1,726\n",
            "ارض        3,648\n",
            "سوريا      3,468\n",
            "وفلسطين   24,067\n",
            "،            219\n",
            "وهولاء    18,637\n",
            "قوم        6,954\n",
            "سامي       9,801\n",
            "##ون       1,733\n",
            "اتخذ       8,795\n",
            "##وا       1,751\n",
            "من         1,726\n",
            "الملاحة   14,002\n",
            "والتجارة  12,923\n",
            "مهنة      16,611\n",
            "لهم        2,819\n",
            "،            219\n",
            "واز        9,254\n",
            "##ده       2,986\n",
            "##رت       1,920\n",
            "حض         5,893\n",
            "##ارتهم   21,923\n",
            "طيلة      14,338\n",
            "250        8,361\n",
            "##0        1,034\n",
            "سنة        2,634\n",
            "تقريبا     6,527\n",
            "(             12\n",
            "من         1,726\n",
            "حوالي      5,543\n",
            "سنة        2,634\n",
            "3000      22,423\n",
            "حتى        2,123\n",
            "سنة        2,634\n",
            "53         7,847\n",
            "##9        1,039\n",
            "ق            245\n",
            ".             18\n",
            "م            248\n",
            ")             13\n",
            ".             18\n",
            "وقد        2,269\n",
            "مرت        3,484\n",
            "على        1,747\n",
            "لبنان      2,851\n",
            "عدة        3,630\n",
            "حض         5,893\n",
            "##ارات     1,912\n",
            "وشع       12,536\n",
            "##وب       1,870\n",
            "استقر     22,122\n",
            "##ت        1,013\n",
            "فيه        2,229\n",
            "منذ        2,597\n",
            "عهد        5,609\n",
            "الفين     23,547\n",
            "##يقي     14,222\n",
            "##ن        1,017\n",
            "،            219\n",
            "مثل        2,358\n",
            "المصريين   8,916\n",
            "القدماء   25,946\n",
            "،            219\n",
            "الاش       2,324\n",
            "##وريين   24,130\n",
            "،            219\n",
            "الفرس     17,551\n",
            "،            219\n",
            "الاغ       3,488\n",
            "##ريق      2,028\n",
            "،            219\n",
            "الرومان   21,225\n",
            "،            219\n",
            "الروم      7,903\n",
            "البي       2,062\n",
            "##زن       3,573\n",
            "##طيين    28,683\n",
            "،            219\n",
            "العرب      3,153\n",
            "،            219\n",
            "الصليب    17,084\n",
            "##يين      2,696\n",
            "،            219\n",
            "الاتراك   20,536\n",
            "العثماني  14,016\n",
            "##ين       1,724\n",
            "،            219\n",
            "فالف      22,314\n",
            "##رن       1,918\n",
            "##سيين    16,084\n",
            ".             18\n",
            "[SEP]          3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)\n",
        "#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)\n",
        "#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "print( [1]*num_seg_b )\n",
        "print ( [0]*num_seg_a )\n",
        "print ( segment_ids)\n",
        "#Segment embeddings help BERT in differentiating a question from the text. zero question , one answer\n",
        "#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "id": "Sy0Aj1uMO_Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678a0fc1-d9cd-4a79-a52f-16974a9954df"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEP token index:  11\n",
            "Number of tokens in segment A:  12\n",
            "Number of tokens in segment B:  104\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now feed this to our model."
      ],
      "metadata": {
        "id": "VVDupdzoWPwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
        "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
      ],
      "metadata": {
        "id": "muCcvzSoWRZN"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the most probable start and end words and providing answers only if the end token is after the start token\n"
      ],
      "metadata": {
        "id": "vR-3K5dhWmYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens with highest start and end scores\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "if answer_end >= answer_start:\n",
        "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "    \n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cK9OaV-WoFF",
        "outputId": "06c4c841-d18c-4677-d8cb-57c3371b2e7c"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            "ماهي المهنة التي كان يمارسها الفينيقيون؟\n",
            "\n",
            "Answer:\n",
            "في القدم ، سكن الفين ##يقي ##ون ارض لبنان الحالية مع جزء من ارض سوريا وفلسطين ، وهولاء قوم سامي ##ون اتخذ ##وا من الملاحة والتجارة مهنة لهم ، واز ##ده ##رت حض ##ارتهم.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = tokens[answer_start]\n",
        "for i in range(answer_start+1, answer_end+1):\n",
        "    if tokens[i][0:2] == \"##\":\n",
        "        answer += tokens[i][2:]\n",
        "    else:\n",
        "        answer += \" \" + tokens[i]\n",
        "        \n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print (\"answer \" + answer )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEWsFMlBmPVx",
        "outputId": "ea65572f-5db0-4cdb-b09d-a2220e345a54"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            "ماهي المهنة التي كان يمارسها الفينيقيون؟\n",
            "answer في القدم ، سكن الفينيقيون ارض لبنان الحالية مع جزء من ارض سوريا وفلسطين ، وهولاء قوم ساميون اتخذوا من الملاحة والتجارة مهنة لهم ، وازدهرت حضارتهم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "r, “Karingu” is a rare word so wordpiece split it into the words, “Karin” and “##gu”.\n",
        "\n",
        "##SUFFIX’ (if any suffix at all — for example, “run”, “##ning”, “##ner”)."
      ],
      "metadata": {
        "id": "M3diwTnTmF5j"
      }
    }
  ]
}